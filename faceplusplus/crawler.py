# -*- coding: iso-8859-1 -*-

"""Module to store crawler classes.

More than one class can be written here, but only one (that specified in the configuration file) will be used by the client to instantiate a crawler object whose :meth:`crawl() <BaseCrawler.crawl>` method will by called to do the collection of the resource received. 

"""

import os
import json
import random
import common
from fpp.facepp import API
from fpp.facepp import APIError


class BaseCrawler:
    """Abstract class. All crawlers should inherit from it or from other class that inherits."""

    def __init__(self, configurationsDictionary):
        """Constructor.  
        
        Upon initialization the crawler object receives everything in the crawler section of the XML configuration file as the parameter *configurationsDictionary*. 
        
        """
        self._extractConfig(configurationsDictionary)
       
    def _extractConfig(self, configurationsDictionary):
        """Extract and store configurations.
        
        If some configuration needs any kind of pre-processing, it is done here. Extend this method if you need to pre-process custom configuration options.
        
        """
        self.config = configurationsDictionary
        if ("echo" not in self.config): self.config["echo"] = {}
    
    def crawl(self, resourceID, filters):
        """Collect the resource.
        
        Must be overriden.
        
        Args:
            * *resourceID* (user defined type): ID of the resource to be collected, sent by the server.
            * *filters* (list): All data (if any) generated by the filters added to server. Sequential filters data come first, in the same order that the filters were specified in the configuration file. Parallel filters data come next, in undetermined order.
            
        Returns:   
            A tuple in the format (*resourceInfo*, *extraInfo*, *newResources*). Any element of the tuple can be ``None``, depending on what the user desires.

            * *resourceInfo* (dict): Resource information dictionary. This information is user defined and must be understood by the persistence handler used. 
            * *extraInfo* (dict): Aditional information. This information is just passed to all filters via :meth:`callback() <filters.BaseFilter.callback>` method and is not used by the server itself. 
            * *newResources* (list): Resources to be stored by the server when the feedback option is enabled. Each new resource is described by a tuple in the format (*resourceID*, *resourceInfo*), where the first element is the resource ID (whose type is defined by the user) and the second element is a dictionary containing resource information (in a format understood by the persistence handler used).
                
        """
        return (None, None, None)


class FPPURLCrawler(BaseCrawler):
    def crawl(self, resourceID, filters):
        echo = common.EchoHandler(self.config)
        
        # Configure data storage directory
        #fppBaseDir = "../../data/fpp"
        fppBaseDir = "../../data/fppselfies"
        fppSubDir = str(int(resourceID.split("_")[0]) % 1000) # OBS: The division here group the media by media ID, instead of group by user ID. To group by user ID it is necessary to change to resourceID.split("_")[1].
        fppDataDir = os.path.join(fppBaseDir, fppSubDir)
        if not os.path.exists(fppDataDir): os.makedirs(fppDataDir)
        
        # Initialize return variables
        #extraInfo = {"mediaerrors": []}
        extraInfo = {"mediaerrors": [], "output": []}
        
        # Check if the file already exists
        fppFilePath = os.path.join(fppDataDir, "%s.json" % resourceID)
        if os.path.isfile(fppFilePath): 
            echo.out(u"Media %s already exists." % resourceID)
            return (None, extraInfo, None)
        
        # Extract filters
        imageURL = filters[0]["data"]["url"]
        application = filters[1]["data"]["application"]
    
        # Get authenticated API object
        apiServer = application["apiserver"]
        apiKey = application["apikey"]
        apiSecret = application["apisecret"]
        api = API(srv = apiServer, key = apiKey, secret = apiSecret, timeout = 60, max_retries = 0, retry_delay = 0)
        echo.out(u"ID: %s (App: %s)." % (resourceID, application["name"]))
        
        # Execute collection
        attributes = ["gender", "age", "race", "smiling", "glass", "pose"]
        try:
            response = api.detection.detect(url = imageURL, attribute = attributes)
        except Exception as error: 
            # HTTP error codes: http://www.faceplusplus.com/detection_detect/
            if isinstance(error, APIError): message = "%d: %s" % (error.code, json.loads(error.body)["error"])
            # socket.error and urllib2.URLError 
            else: message = str(error)
            extraInfo["mediaerrors"].append((resourceID, {"error": message}))
        else: 
            with open(fppFilePath, "w") as fppFile: json.dump(response, fppFile)
            
            for face in response["face"]:
                faceInfo = filters[0]["data"]
                faceInfo["gender_val"] = face["attribute"]["gender"]["value"]
                faceInfo["gender_cnf"] = face["attribute"]["gender"]["confidence"]
                faceInfo["race_val"] = face["attribute"]["race"]["value"]
                faceInfo["race_cnf"] = face["attribute"]["race"]["confidence"]
                faceInfo["smile"] = face["attribute"]["smiling"]["value"]
                faceInfo["age_val"] = face["attribute"]["age"]["value"]
                faceInfo["age_rng"] = face["attribute"]["age"]["range"]
                extraInfo["output"].append((resourceID, faceInfo))
        
        return (None, extraInfo, None)

        
class FPPRandomMediaCrawler(BaseCrawler):  
    def crawl(self, resourceID, filters):
        echo = common.EchoHandler(self.config)
        echo.out(u"User ID received: %s." % resourceID)
        
        # Extract filters
        application = filters[0]["data"]["application"]
    
        # Get authenticated API object
        apiServer = application["apiserver"]
        apiKey = application["apikey"]
        apiSecret = application["apisecret"]
        api = API(srv = apiServer, key = apiKey, secret = apiSecret, timeout = 5, max_retries = 0, retry_delay = 0)
        echo.out(u"App: %s." % str(application["name"]))
    
        # Configure data storage directory
        fppBaseDir = "../../data/fpp"
        fppDataDir = os.path.join(fppBaseDir, str(resourceID % 1000), str(resourceID))
        if not os.path.exists(fppDataDir): os.makedirs(fppDataDir)
        
        # Load user feed file
        feedsBaseDir = "../../data/feeds"
        feedsFilePath = os.path.join(feedsBaseDir, str(resourceID % 1000), "%s.feed" % resourceID)
        with open(feedsFilePath, "r") as feedFile: feed = json.load(feedFile)
        
        # Get random media sample to collect
        feedSampleSize = 10
        feedList = random.sample(feed, min(len(feed), feedSampleSize))
        
        # Initialize return variables
        extraInfo = {"mediaerrors": [], "usersok": [], "usersproblem": []}
        
        # Execute collection
        attributes = ["gender", "age", "race", "smiling", "glass", "pose"]
        for i, media in enumerate(feedList):
            echo.out(u"Collecting media %d." % (i + 1))
            try:
                response = api.detection.detect(url = media["images"]["low_resolution"]["url"], attribute = attributes)
            except Exception as error: 
                # HTTP error codes: http://www.faceplusplus.com/detection_detect/
                if isinstance(error, APIError): message = "%d: %s" % (error.code, json.loads(error.body)["error"])
                # socket.error and urllib2.URLError 
                else: message = str(error)
                extraInfo["mediaerrors"].append((media["id"], {"error": message}))
                extraInfo["usersproblem"].append((resourceID, None))
            else:
                fppFilePath = os.path.join(fppDataDir, "%s.fpp" % media["id"])
                with open(fppFilePath, "w") as fppFile: json.dump(response, fppFile)
                    
        extraInfo["usersok"].append((resourceID, None))
        return (None, extraInfo, None)
      