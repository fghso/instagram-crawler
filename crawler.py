# -*- coding: iso-8859-1 -*-

import os
import time
import logging


class Crawler:
    # Return a name that identifies the crawler
    def getName(self):
        return os.getpid()

    # This is the method that effectively does the collection. The method receives as arguments the ID of the resource to 
    # be collected, the logging option configured in the client (true or false) and all data (if any) generated by the 
    # filters added to the server. The return value must be a tuple containing a resource information dictionary at the 
    # first position. This information is user defined and must be understood by the persistence handler used. If the 
    # feedback option is enabled, the resources to be stored can be given in a list as the second element of the tuple. 
    # Each new resource is described by a tuple, where the first element is the resource ID and the second element is a 
    # dictionary containing resource information (also in a format understood by the persistence handler used).
    def crawl(self, resourceID, loggingActive, filters):
        if (loggingActive):
            logging.info("Resource received: %s" % resourceID)
            logging.info("Filters: %s" % filters)

        print "Resource received: %s" % resourceID        
        print "Filters: %s" % filters
        
        sleepTime = 30
        print "Sleeping for %d seconds..." % sleepTime
        time.sleep(sleepTime)
        print "Awaked!"
        
        newResources = []
        newResources.append((resourceID + 4, {"responsecode": 5, "annotation": "NEW"}))
        newResources.append((resourceID + 6, {"responsecode": 5, "annotation": "NEW"}))
        
        return ({"responsecode": 3, "annotation": "OK"}, newResources)
